{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSW_8VBadO6T",
        "outputId": "4b21cebc-069e-44d1-dfed-bc9aaa093c28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-15a7fc67f77c>:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data.dropna(subset=['CustomerID'], inplace=True)\n",
            "<ipython-input-2-15a7fc67f77c>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['CustomerID'] = data['CustomerID'].astype(int)\n",
            "<ipython-input-2-15a7fc67f77c>:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['interaction'] = 1\n",
            "<ipython-input-2-15a7fc67f77c>:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data.rename(columns={'CustomerID': 'user_id', 'StockCode': 'item_id'}, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\"\n",
        "data = pd.read_excel(url)\n",
        "\n",
        "# Preprocess the data\n",
        "data['date'] = pd.to_datetime(data['InvoiceDate'])\n",
        "data = data[['CustomerID', 'StockCode', 'Description', 'Quantity', 'UnitPrice', 'date', 'Country']]\n",
        "data.dropna(subset=['CustomerID'], inplace=True)\n",
        "data['CustomerID'] = data['CustomerID'].astype(int)\n",
        "data['interaction'] = 1\n",
        "\n",
        "# Rename columns for consistency\n",
        "data.rename(columns={'CustomerID': 'user_id', 'StockCode': 'item_id'}, inplace=True)\n",
        "\n",
        "# Create mappings for item descriptions\n",
        "item_description_mapping = data.set_index('item_id')['Description'].to_dict()\n",
        "\n",
        "# Extract popular products for each location\n",
        "popular_products = data.groupby(['Country', 'item_id'])['Quantity'].sum().reset_index()\n",
        "popular_products = popular_products.sort_values(by=['Country', 'Quantity'], ascending=[True, False])\n",
        "\n",
        "# Create a mapping of popular products for each location\n",
        "location_popular_products = {}\n",
        "for location, group in popular_products.groupby('Country'):\n",
        "    location_popular_products[location] = group['item_id'].tolist()\n",
        "\n",
        "# Convert item_id to indices\n",
        "item_mapping = {item: idx for idx, item in enumerate(data['item_id'].unique())}\n",
        "inverse_item_mapping = {idx: item for item, idx in item_mapping.items()}  # Add inverse mapping\n",
        "data['item_id'] = data['item_id'].map(item_mapping)\n",
        "\n",
        "# Split data into training, validation, and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_data, temp_data = train_test_split(data, test_size=0.4, random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Prepare training tensor\n",
        "user_mapping = {user: idx for idx, user in enumerate(train_data['user_id'].unique())}\n",
        "num_users = len(user_mapping)\n",
        "\n",
        "input_dim = len(item_mapping)\n",
        "context_dim = 10  # We'll use the top 10 popular products as context\n",
        "\n",
        "train_tensor = torch.zeros((num_users, input_dim))\n",
        "context_tensor = torch.zeros((num_users, context_dim))\n",
        "\n",
        "for row in train_data.itertuples():\n",
        "    user_idx = user_mapping[row.user_id]\n",
        "    item_idx = row.item_id\n",
        "    train_tensor[user_idx, item_idx] = 1\n",
        "    location_context = [1 if item in location_popular_products[row.Country][:10] else 0 for item in range(input_dim)]\n",
        "    context_tensor[user_idx] = torch.tensor(location_context[:context_dim], dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim, context_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim + context_dim, hidden_dim)\n",
        "        self.fc2_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc3 = nn.Linear(latent_dim + context_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = torch.relu(self.fc1(x))\n",
        "        return self.fc2_mu(h), self.fc2_logvar(h)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = torch.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h))\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        xc = torch.cat([x, context], dim=1)\n",
        "        mu, logvar = self.encode(xc)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        zc = torch.cat([z, context], dim=1)\n",
        "        return self.decode(zc), mu, logvar\n",
        "\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_dim = 256\n",
        "latent_dim = 20\n",
        "\n",
        "# Initialize model\n",
        "vae = VAE(input_dim, hidden_dim, latent_dim, context_dim)\n",
        "\n",
        "# Loss and optimizer\n",
        "optimizer = optim.Adam(vae.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "et-vGox6flBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 50\n",
        "batch_size = 64\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    vae.train()\n",
        "    train_loss = 0\n",
        "    for i in range(0, len(train_tensor), batch_size):\n",
        "        batch_x = train_tensor[i:i + batch_size]\n",
        "        batch_context = context_tensor[i:i + batch_size]\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = vae(batch_x, batch_context)\n",
        "        loss = loss_function(recon_batch, batch_x, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss / len(train_tensor)}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNhPdweGfqtl",
        "outputId": "4adf395c-8f61-4062-ddbf-452d72dc06cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 166.9732585044859\n",
            "Epoch 2/50, Loss: 166.62394762006093\n",
            "Epoch 3/50, Loss: 166.27410784193466\n",
            "Epoch 4/50, Loss: 165.73321176705855\n",
            "Epoch 5/50, Loss: 165.970474853798\n",
            "Epoch 6/50, Loss: 165.5745596884578\n",
            "Epoch 7/50, Loss: 165.34919682647683\n",
            "Epoch 8/50, Loss: 163.78335231734238\n",
            "Epoch 9/50, Loss: 163.2353421029685\n",
            "Epoch 10/50, Loss: 162.76798981228222\n",
            "Epoch 11/50, Loss: 162.19341408497698\n",
            "Epoch 12/50, Loss: 161.79707867642009\n",
            "Epoch 13/50, Loss: 161.23699145699828\n",
            "Epoch 14/50, Loss: 160.55122158413212\n",
            "Epoch 15/50, Loss: 160.05802058710313\n",
            "Epoch 16/50, Loss: 159.85442534955652\n",
            "Epoch 17/50, Loss: 159.46750354435937\n",
            "Epoch 18/50, Loss: 158.68693330346946\n",
            "Epoch 19/50, Loss: 158.0278507597105\n",
            "Epoch 20/50, Loss: 157.6109414560608\n",
            "Epoch 21/50, Loss: 156.85996475237374\n",
            "Epoch 22/50, Loss: 156.38450238176884\n",
            "Epoch 23/50, Loss: 155.71518919938364\n",
            "Epoch 24/50, Loss: 155.18352744856725\n",
            "Epoch 25/50, Loss: 154.49213196407885\n",
            "Epoch 26/50, Loss: 154.19226815450696\n",
            "Epoch 27/50, Loss: 154.2560372636077\n",
            "Epoch 28/50, Loss: 155.21235460841268\n",
            "Epoch 29/50, Loss: 154.0001594989961\n",
            "Epoch 30/50, Loss: 152.50880874397302\n",
            "Epoch 31/50, Loss: 151.52095202927563\n",
            "Epoch 32/50, Loss: 150.8401350894516\n",
            "Epoch 33/50, Loss: 150.23327835086536\n",
            "Epoch 34/50, Loss: 149.46292880903425\n",
            "Epoch 35/50, Loss: 148.82419371753826\n",
            "Epoch 36/50, Loss: 148.31723345073317\n",
            "Epoch 37/50, Loss: 147.89341087438373\n",
            "Epoch 38/50, Loss: 147.37715729557473\n",
            "Epoch 39/50, Loss: 146.82915415467141\n",
            "Epoch 40/50, Loss: 146.45492856557132\n",
            "Epoch 41/50, Loss: 146.40247675524856\n",
            "Epoch 42/50, Loss: 145.53228927778198\n",
            "Epoch 43/50, Loss: 145.04610812848787\n",
            "Epoch 44/50, Loss: 144.57792967479315\n",
            "Epoch 45/50, Loss: 143.72856001420453\n",
            "Epoch 46/50, Loss: 142.91540273207082\n",
            "Epoch 47/50, Loss: 142.23222342250028\n",
            "Epoch 48/50, Loss: 141.82723889603483\n",
            "Epoch 49/50, Loss: 141.56424512825657\n",
            "Epoch 50/50, Loss: 140.86021579704487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Function to generate recommendations for a new user based on location\n",
        "def generate_recommendations_for_new_user(vae, location, item_mapping, location_popular_products, context_dim):\n",
        "    vae.eval()\n",
        "    # Create location context with the correct dimensionality\n",
        "    location_context = [1 if item in location_popular_products[location][:10] else 0 for item in range(context_dim)]\n",
        "    location_context_tensor = torch.tensor(location_context, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    # Create a zero vector for the new user with the correct dimensionality\n",
        "    new_user_vector = torch.zeros((1, input_dim))  # Use the global variable input_dim\n",
        "\n",
        "    # Generate recommendations\n",
        "    with torch.no_grad():\n",
        "        recon_vector, _, _ = vae(new_user_vector, location_context_tensor)\n",
        "\n",
        "    # Map item indices back to item IDs\n",
        "    recon_scores = recon_vector.squeeze().numpy()\n",
        "    item_indices = np.argsort(recon_scores)[::-1]\n",
        "    recommended_items = [inverse_item_mapping[idx] for idx in item_indices[:10]]\n",
        "\n",
        "    return recommended_items\n",
        "\n",
        "# Example usage\n",
        "new_user_location = \"United Kingdom\"  # Specify the new user's location\n",
        "# Pass context_dim as an argument\n",
        "recommended_items = generate_recommendations_for_new_user(vae, new_user_location, item_mapping, location_popular_products, context_dim)\n",
        "# Map item IDs back to descriptions\n",
        "actual_recommended_items = [item_description_mapping[item] for item in recommended_items]\n",
        "\n",
        "print(f\"Top 10 recommended items for a new user in {new_user_location}: {actual_recommended_items}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWvP1WZbgpUu",
        "outputId": "ca5ce529-899b-436b-fd58-2e01ce944ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 recommended items for a new user in United Kingdom: ['CINDERELLA CHANDELIER ', 'DOORMAT UNION FLAG', 'EDWARDIAN PARASOL BLACK', 'RECORD FRAME 7\" SINGLE SIZE ', 'RECYCLED ACAPULCO MAT GREEN', 'RECYCLED ACAPULCO MAT PINK', 'SET OF 16 VINTAGE BLACK CUTLERY', 'CHILLI LIGHTS', 'CARRIAGE', 'FLOOR CUSHION ELEPHANT CARNIVAL']\n"
          ]
        }
      ]
    }
  ]
}